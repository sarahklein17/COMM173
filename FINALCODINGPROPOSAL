#gathering YouTube videos
from selenium import webdriver
import pandas as pd
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys
import time
url = 'https://www.youtube.com/results?search_query=andrew+tate+chicken'
driver = webdriver.Chrome()
driver.get(url)
def scroll_to_bottom(driver):
    old_position = 0
    new_position = None
    
    while (new_position != old_position):
        old_position = driver.execute_script(("return (window.pageYOffset !== undefined) ?"
                                            " window.pageYOffset : (document.documentElement ||"
                                            " document.body.parentNode || document.body);"))
        time.sleep(5)
        driver.execute_script(("var scrollingElement = (document.scrollingElement ||"
                              " document.body);scrollingElement.scrollTop = "
                              " scrollingElement.scrollHeight"))
        
        time.sleep(5)
        new_position = driver.execute_script(("return (window.pageYOffset !== undefined) ?"
                                             " window.pageYOffset : (document.documentElement ||"
                                             " document.body.parentNode || documentBody);"))
scroll_to_bottom(driver)
user_data = driver.find_elements(by=By.XPATH,value='//*[@id="video-title"]')
print(len(user_data))
671
links = []
for i in user_data:
    if (i.get_attribute('href') != None):
        links.append(i.get_attribute('href'))

print(len(links))
564
df = pd.DataFrame(columns = ['link', 'title', 'description', 'category'])
v_category = "andrew_tate"
wait = WebDriverWait(driver, 50)
for x in links:
    driver.get(x)
    v_id = x
    v_title = wait.until(EC.presence_of_element_located(
                   (By.CSS_SELECTOR,"h1.style-scope.ytd-watch-metadata yt-formatted-string"))).text

    v_description =  wait.until(EC.presence_of_element_located(
                                (By.CSS_SELECTOR,"div#snippet yt-formatted-string"))).text
    df.loc[len(df)] = [v_id, v_title, v_description, v_category]
driver.quit()
print(df)
      
df.to_csv('comm173_youtube.csv', index=False)


#Cleaning this Data Set

import pandas as pd
df = pd.read_csv("youtube_dataset.csv")
df.dropna(inplace=True)
df['title'] = df['title'].str.strip()
df['description'] = df['description'].str.strip()
df['category'] = df['category'].str.strip()
df['link'] = df['link'].str.lower()
df.drop_duplicates(inplace=True)
print(df)



#gathering YouTube comments

from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
df = pd.read_csv('comm173_youtube2.csv')
print(df)
df[~df.link.str.contains("/shorts")]
df_2 = df[~df.link.str.contains("/shorts")]
driver = webdriver.Chrome(('/Users/sarahklein/Documents/COMM173/chromedriver'))
driver = webdriver.Chrome(('/Users/davidjeong/Documents/COMM170/chromedriver'))
driver.maximize_window()
driver.implicitly_wait(20)
comments = []
comment_likes = []
def scroll_to_bottom(driver):
    old_position = 0
    new_position = None
    scroll_distance = 500
    
    while (new_position != old_position):
        old_position = driver.execute_script(("return (window.pageYOffset !== undefined) ?"
                                            " window.pageYOffset : (document.documentElement ||"
                                            " document.body.parentNode || document.body);"))
        time.sleep(5)
        driver.execute_script(("var scrollingElement = (document.scrollingElement ||"
                              " document.body);scrollingElement.scrollTop = "
                              " scrollingElement.scrollHeight"))
        
        time.sleep(5)
        new_position = driver.execute_script(("return (window.pageYOffset !== undefined) ?"
                                             " window.pageYOffset : (document.documentElement ||"
                                             " document.body.parentNode || documentBody);"))
for index, row in df_2.iterrows():
    driver.get(row['link'])

    wait = WebDriverWait(driver, 10)
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '#comments #content-text')))
    
    scroll_to_bottom(driver)
    
    time.sleep(2)

    comment_elems = driver.find_elements(By.CSS_SELECTOR,'#comments #content-text')
    comment_like_elems = driver.find_elements(By.CSS_SELECTOR,'#comments #vote-count-middle')
    comment_text = [elem.text for elem in comment_elems]
    comment_like_text = [elem.text for elem in comment_like_elems]

    for i in range(len(comment_text)):
        comments.append([row['link'], row['title'], comment_text[i]])
        comment_likes.append([row['link'], row['title'], comment_text[i], comment_like_text[i]])
driver.quit()
comments_df = pd.DataFrame(comments, columns=['Link', 'Title', 'Comment'])
comment_likes_df = pd.DataFrame(comment_likes, columns=['Link', 'Title', 'Comment', 'Likes'])
comments_df.to_csv('video_comments.csv', index=False)
comment_likes_df.to_csv('video_comment_likes.csv', index=False)






#Filter out the comments using NLTK - would do again with "females"
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

def get_comments_with_word(dataset, female):
    selected_comments = []
    
    for video in dataset:
        comments = video['comments']
        for comment in comments:
     
            words = word_tokenize(comment)
            
            if word in words:
                selected_comments.append(comment)
    
    return selected_comments

dataset = [
    {
        'video_id': '123',
        'comments': [  ]
   

desired_word = 'female'
selected_comments = get_comments_with_word(AndrewTate.csv, female)

for comment in selected_comments:
    print(comment)



#Sentiment Analysis using NLTK
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

nltk.download('vader_lexicon')

sid = SentimentIntensityAnalyzer()

selected_comments = []

def detect_misogyny(comment):
    sentiment_scores = sid.polarity_scores(comment)
    if sentiment_scores['compound'] < 0 or 'female' in comment.lower() or 'females' in comment.lower():
        return True
    else:
        return False

for comment in selected_comments:
    if detect_misogyny(comment):
        print("Misogyny or negative sentiment detected:", comment)
